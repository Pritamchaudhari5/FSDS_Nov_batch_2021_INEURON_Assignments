{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 21 Solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "#### 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "938b44b8",
   "metadata": {},
   "source": [
    "**Solution:** The estimated depth of a decision tree trained on an unrestricted one million instance training set can vary widely depending on various factors. Here are the main points:\n",
    "\n",
    "- The depth of a decision tree indicates the number of levels or splits in the tree and determines its complexity.\n",
    "- For a one million instance training set, the estimated depth of a decision tree can range anywhere from a few levels to hundreds of levels, depending on multiple factors.\n",
    "- The complexity of the underlying data can affect the maximum depth required to accurately represent the patterns, with more complex data requiring greater depth.\n",
    "- The level of regularization applied to the decision tree model can also impact the maximum depth, with less regularization allowing for deeper trees.\n",
    "- Additionally, other hyperparameters, such as the minimum number of samples per leaf or the maximum number of features allowed per split, can influence the depth of the decision tree.\n",
    "- In general, the optimal maximum depth for a decision tree should be determined through a process of validation and tuning of hyperparameters to optimize model performance and generalization accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "#### 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "697eef4e",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "The Gini impurity of a node is usually lower than or equal to that of its parent. Here are the main points:\n",
    "\n",
    "- Gini impurity is a measure of impurity or uncertainty in a decision tree node and represents the probability of incorrectly classifying a sample in that node.\n",
    "- Each split in a decision tree algorithm creates two child nodes from a parent node, with each child node representing a subset of the parent node's samples.\n",
    "- The goal of a decision tree algorithm is to create splits that minimize the impurity in the resulting child nodes, which leads to a more accurate classification model.\n",
    "- Since each split in a decision tree algorithm is designed to decrease the impurity of the resulting child nodes, it's usually the case that the Gini impurity in a child node is lower than or equal to that of its parent node.\n",
    "- However, it's possible for the impurity to increase in some cases, such as if the split results in a larger subset of data with a more even distribution of classes.\n",
    "- In practice, the impurity tends to decrease with each level of the decision tree, resulting in a hierarchy of increasingly pure nodes as we move down the tree."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "#### 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f75896f",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "- Overfitting occurs when the decision tree algorithm becomes overly complex and captures noise in the training data, leading to poor generalization performance on unseen data.\n",
    "- One common technique to address overfitting in decision trees is to reduce the maximum depth of the tree.\n",
    "- By reducing the maximum depth, the decision tree becomes less complex, resulting in a simpler model that is less likely to overfit the training data.\n",
    "- Reducing the maximum depth prevents the decision tree algorithm from creating too many splits, which can lead to overfitting by allowing it to memorize the training data instead of finding the underlying patterns in the data.\n",
    "- It's important to note that reducing the maximum depth too much can also result in underfitting, where the decision tree model is too simple to capture the underlying patterns in the data.\n",
    "- Finding the optimal maximum depth involves a trade-off between model complexity and model performance, so it's important to use cross-validation techniques to determine the best hyperparameters for the decision tree."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "#### 4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa4a8cfe",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "- Scaling the input features in a decision tree is generally not necessary, as decision trees are not sensitive to the scale of the features.\n",
    "- Underfitting in a decision tree occurs when the model is not complex enough to capture the underlying patterns in the data.\n",
    "- Scaling input features is unlikely to help with underfitting in a decision tree since decision trees do not rely on the distance between samples.\n",
    "- To address underfitting in a decision tree, you might try increasing the maximum depth, adjusting hyperparameters such as the minimum number of samples per leaf or the maximum number of features allowed per split, or adding more features or data to provide more information to the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "#### 5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da064baf",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "- Training a decision tree on a training set of 10 million instances will likely take longer than training on a smaller dataset.\n",
    "- The exact amount of time it will take to train on a dataset of this size depends on various factors such as the complexity of the data, the chosen algorithm or implementation of the decision tree, and available computing resources.\n",
    "- One way to estimate the time it will take to train is to assume that the time it takes to train the model scales linearly with the size of the dataset.\n",
    "- If it takes an hour to train a decision tree on a training set with 1 million instances, it may take around 10 hours to train on a training set with 10 million instances, assuming that all other factors remain constant.\n",
    "- However, it's important to note that training on a larger dataset may also require more memory and computational resources, which could further increase the training time.\n",
    "- Using parallel computing techniques or distributed computing frameworks can help reduce the training time for large datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "#### 6. Will setting presort=True speed up training if your training set has 100,000 instances ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61bb5f8f",
   "metadata": {},
   "source": [
    "**Solution:** The `presort=True` parameter in decision trees specifies whether the training data should be presorted prior to training the model. Sorting the data can speed up the training process for small datasets, but for larger datasets, it can actually slow down the training time and increase memory usage. \n",
    "\n",
    "Here are the main points:\n",
    "\n",
    "- Setting `presort=True` can speed up training for small datasets by reducing the time taken to find the best split at each node of the decision tree.\n",
    "- Generally, if the training set has fewer than 10,000 instances, setting `presort=True` may lead to faster training times.\n",
    "- However, if the training set has 100,000 instances, sorting the data in memory can become very memory-intensive and can actually slow down the training time, especially if the machine's RAM is limited.\n",
    "- Therefore, for datasets with more than 10,000 instances, it is generally recommended to set `presort=False` to avoid slowing down the training time and increasing memory usage.\n",
    "- If the decision tree implementation supports parallelization, enabling parallel execution (e.g., setting the `n_jobs` parameter to a value greater than 1) can help improve training speed for larger datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "1. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "2. Divide the dataset into a training and a test collection with train test split().\n",
    "3. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "4. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fd9fb43",
   "metadata": {},
   "source": [
    "##### **Solution:** Sure, here are the steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "a. Create the moons dataset with make_moons function from sklearn.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "507ebbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4a1152d",
   "metadata": {},
   "source": [
    "b. Split the dataset into a training and test set using train_test_split from sklearn.model_selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8860b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0a0e14c",
   "metadata": {},
   "source": [
    "c. Use grid search with cross-validation to find the optimal hyperparameters. In this example, we'll search over different values of max_leaf_nodes using GridSearchCV from sklearn.model_selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e80ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_leaf_nodes': 32}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_leaf_nodes': [2, 4, 8, 16, 32, 64, 128]}\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dtc, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "343ee696",
   "metadata": {},
   "source": [
    "d. Train the model with the optimal hyperparameters found in the previous step and assess its performance on the test set, using accuracy_score from sklearn.metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c22b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 86.95%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42)\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. Follow these steps to grow a forest:\n",
    "1. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.\n",
    "2. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision        Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "3. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "4. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "056c47cf",
   "metadata": {},
   "source": [
    "**Solution:** Sure, here are the steps to grow a forest:\n",
    "\n",
    "a. Create 1,000 subsets of the training set, each containing 100 instances chosen at random using ShuffleSplit from sklearn.model_selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948b7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "forest_X = []\n",
    "forest_y = []\n",
    "ss = ShuffleSplit(n_splits=n_trees, test_size=n_instances, random_state=42)\n",
    "for train_index, _ in ss.split(X_train):\n",
    "    forest_X.append(X_train[train_index])\n",
    "    forest_y.append(y_train[train_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f72a023",
   "metadata": {},
   "source": [
    "b. Train one decision tree on each subset with the best hyperparameters found in the previous exercise, and compute its accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a671321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 86.90%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "forest = []\n",
    "for i in range(n_trees):\n",
    "    dtc = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42)\n",
    "    dtc.fit(forest_X[i], forest_y[i])\n",
    "    forest.append(dtc)\n",
    "\n",
    "y_pred = np.array([dtc.predict(X_test) for dtc in forest])\n",
    "\n",
    "y_pred_majority_votes, _ = mode(y_pred, axis=0)\n",
    "forest_acc = accuracy_score(y_test, y_pred_majority_votes.flatten())\n",
    "print(\"Random forest accuracy: {:.2f}%\".format(forest_acc * 100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61379c61",
   "metadata": {},
   "source": [
    "In this code, we first create a forest by training one decision tree on each subset of the training set. We store these trees in a list called forest. Then, we use the predict method of each decision tree to make predictions on the test set, and store these predictions in a numpy array y_pred. Finally, we compute the majority vote for each test instance and compute the forest's accuracy on the test set using accuracy_score from sklearn.metrics.\n",
    "\n",
    "With these steps, we should be able to grow a Random Forest classifier on the given dataset. The accuracy of the classifier will depend on the specific dataset and problem, and we can explore other hyperparameters or regularization techniques to further improve the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
